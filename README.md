# Transformers in Vision Course - HSLU

Welcome to the GitHub repository for the "Transformers in Vision" course at HSLU.

This repo contains materials and hands-on exercises for Day 3 and Day 4, from the bootcamp starting August 25, 2026.

![Vision-Transformer](https://github.com/safouaneelg/HSLU-Transformers-in-Vision/blob/main/vit-arch-recap.gif?raw=true)

## Course PDF presentation

You can download the PDF file from [this link](/DAY 3 & 4 - course Vision Transformers.pdf)

## Hands-On Sessions Recap

### Day 1

- Pre-trained Classification Model: Utilized "Torch" and "timm" frameworks to explore a pre-trained Vision Transformer (ViT) model for classification tasks.

- Visualization of Features Map: Visualized feature maps to understand the internal representations of ViT models.

- Finetuning ViT on Classification Task: Fine-tuned a ViT model on a classification dataset using the same frameworks.

- Inferencing Pre-trained Model: Explored inferencing with Ultralytics (YOLO models) on a pre-trained model.

- Finetuning YOLO on Custom Dataset: Adjusted YOLOv10 for a custom dataset using Ultralytics.

### Day 2

- Inference CLIP for Zero-Shot Classification: Applied CLIP for zero-shot classification tasks.

- GroundingDINO for Zero-Shot Detection: Used GroundingDINO for zero-shot object detection.

- Prompt Engineering: Experimented with prompt engineering techniques for better model outputs.

- Stable-Diffusion: Implemented Stable-Diffusion for image generation with prompt engineering.

- BLIP Image Captioning: Utilized BLIP for generating image captions.

### Getting Started

1. Clone this repository:

```bash
git clone https://github.com/safouaneelg/HSLU-Transformers-in-Vision.git
```

2. The python notebooks can be accessed in the subfolder `notebooks`

