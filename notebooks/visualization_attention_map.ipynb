{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/14xNi1SJZm17TBc6rKaWvOJYwx8wUOTcK)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Author: \n",
        "- **Safouane El Ghazouali**, \n",
        "- Ph.D. in AI, \n",
        "- Senior data scientist and researcher at TOELT LLC,\n",
        "- Lecturer at HSLU\n",
        "\n",
        "# -----  -----  -----  -----  -----  -----  -----  -----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "intro_attention_maps"
      },
      "source": [
        "# Visualizing Attention Maps in Transformers\n",
        "\n",
        "Welcome to this hands-on notebook on visualizing attention maps in Vision Transformers (ViTs)! Attention maps are a powerful tool to understand how transformers focus on different parts of an input image, revealing what the model 'pays attention to' during inference.\n",
        "\n",
        "![Attention Map Example](https://www.researchgate.net/publication/353612417/figure/fig3/AS:1051863327727618@1627795168385/The-output-attention-map-of-the-base-model-and-the-multi-attention-guided-method-on.png)\n",
        "\n",
        "### Why Visualize Attention Maps?\n",
        "- **Interpretability**: Understand which parts of an image the model focuses on.\n",
        "- **Debugging**: Identify if the model is attending to irrelevant regions.\n",
        "- **Insight into Transformers**: See the attention mechanism in action."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "learning_objectives"
      },
      "source": [
        "### What You'll Learn\n",
        "- How attention maps are generated in Vision Transformers.\n",
        "- Extracting attention weights from a pre-trained ViT model.\n",
        "- Visualizing attention maps overlaid on images.\n",
        "- Interpreting the results.\n",
        "- Exploring and inspecting the model structure for better understanding."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "env_setup"
      },
      "source": [
        "# 🧰 Environment Setup\n",
        "\n",
        "We'll use PyTorch and the `timm` library to load a pre-trained Vision Transformer and Matplotlib for visualization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_deps"
      },
      "outputs": [],
      "source": [
        "!pip install -q torch torchvision timm matplotlib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imports"
      },
      "source": [
        "### Import Libraries\n",
        "\n",
        "We import the necessary libraries for loading the model, processing images, and visualizing attention maps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "import_libs"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import timm\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import requests\n",
        "import torch.nn.functional as F\n",
        "\n",
        "print(\"PyTorch version:\", torch.__version__)\n",
        "print(\"timm version:\", timm.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "load_model"
      },
      "source": [
        "# 📦 Loading a Pre-trained Vision Transformer\n",
        "\n",
        "We'll use a pre-trained Vision Transformer (ViT) model from `timm`. The model is pre-trained on ImageNet."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create_model"
      },
      "outputs": [],
      "source": [
        "model = timm.create_model('vit_base_patch16_224', pretrained=True, num_classes=1000)\n",
        "model.eval()\n",
        "\n",
        "# Explanation\n",
        "# model: A pre-trained Vision Transformer with 16x16 patch size and 224x224 input resolution.\n",
        "# eval(): Sets the model to evaluation mode for inference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import timm\n",
        "\n",
        "# Create the model\n",
        "model = timm.create_model('vit_base_patch16_224', pretrained=True)\n",
        "\n",
        "# Access the model's configuration\n",
        "config = timm.data.resolve_data_config(model.pretrained_cfg)\n",
        "\n",
        "# Print the input size\n",
        "print(f\"Model: {model.pretrained_cfg['architecture']}\")\n",
        "print(f\"Input size: {config['input_size']}\")\n",
        "print(f\"Normalization mean: {config['mean']}\")\n",
        "print(f\"Normalization std: {config['std']}\")\n",
        "\n",
        "num_classes = model.head.out_features\n",
        "print(f\"The model is configured to predict {num_classes} classes.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "explore_model"
      },
      "source": [
        "# 🔎 Exploring the Model Structure\n",
        "\n",
        "Before diving into attention visualization, it's important to understand the model's architecture. As a data scientist, inspecting models helps in debugging, customizing, and understanding how they work internally.\n",
        "\n",
        "### Why Explore Models?\n",
        "- Identify layers and blocks for modification or hooking.\n",
        "- Check dimensions, parameters, and configurations (e.g., number of heads).\n",
        "- Facilitate transfer learning or fine-tuning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "print_model"
      },
      "source": [
        "### Printing the Entire Model\n",
        "\n",
        "Printing the model gives a high-level overview of its components: patch embedding, positional embedding, transformer blocks, normalization, and head."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "print_model_code"
      },
      "outputs": [],
      "source": [
        "print(model)\n",
        "\n",
        "# Explanation\n",
        "# This displays the hierarchical structure of the ViT model, including the sequence of transformer blocks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "access_blocks"
      },
      "source": [
        "### Accessing Specific Blocks\n",
        "\n",
        "The transformer consists of multiple blocks (layers). We can access them individually to inspect or modify."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "access_blocks_code"
      },
      "outputs": [],
      "source": [
        "# Number of transformer blocks\n",
        "num_blocks = len(model.blocks)\n",
        "print(f\"Number of transformer blocks: {num_blocks}\")\n",
        "\n",
        "# Access the first block\n",
        "first_block = model.blocks[0]\n",
        "print(\"\\nFirst transformer block:\")\n",
        "print(first_block)\n",
        "print(\"\\------------------------/\")\n",
        "# Access the last block\n",
        "last_block = model.blocks[-1]\n",
        "print(\"\\nLast transformer block:\")\n",
        "print(last_block)\n",
        "\n",
        "# Explanation\n",
        "# model.blocks: A list of Block modules, each containing attention, MLP, and normalization layers.\n",
        "# We often focus on the last block for attention visualization as it captures high-level features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "access_layers"
      },
      "source": [
        "### Accessing Layers Within a Block\n",
        "\n",
        "Each block has sub-layers like attention (attn), MLP, and norms. Accessing them allows fine-grained control, e.g., for hooking or parameter inspection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "access_layers_code"
      },
      "outputs": [],
      "source": [
        "# Access the attention module in the last block\n",
        "attn_module = last_block.attn\n",
        "print(\"Attention module in last block:\")\n",
        "print(attn_module)\n",
        "\n",
        "# Get key attributes\n",
        "print(f\"Number of attention heads: {attn_module.num_heads}\")\n",
        "print(f\"Head dimension: {attn_module.head_dim}\")\n",
        "\n",
        "# Access the QKV linear layer\n",
        "qkv_layer = attn_module.qkv\n",
        "print(\"\\nQKV linear layer:\")\n",
        "print(qkv_layer)\n",
        "\n",
        "# Explanation\n",
        "# attn_module: The Attention submodule handling self-attention.\n",
        "# num_heads: Number of parallel attention heads (12 for ViT-Base).\n",
        "# qkv: Linear layer projecting input to queries, keys, values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "other_exploration"
      },
      "source": [
        "### Other Useful Exploration Techniques\n",
        "- **List attributes/methods**: Use `dir(object)` to see available properties, e.g., `dir(model)`.\n",
        "- **Parameter count**: `sum(p.numel() for p in model.parameters())`.\n",
        "- **Layer names**: Use `model.named_modules()` to iterate over all modules with names."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "param_count"
      },
      "outputs": [],
      "source": [
        "# Total number of parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Total parameters: {total_params:,}\")\n",
        "\n",
        "# List some module names\n",
        "for name, module in list(model.named_modules())[:5]:\n",
        "    print(name, module)\n",
        "\n",
        "# Explanation\n",
        "# total_params: Helps gauge model size and complexity.\n",
        "# named_modules(): Useful for targeting specific layers by name."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "image_preprocessing"
      },
      "source": [
        "# 🖼️ Image Preprocessing\n",
        "\n",
        "We need to preprocess an input image to match the model's expected input format (224x224 pixels, normalized)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "preprocess_image"
      },
      "outputs": [],
      "source": [
        "# Download a sample image\n",
        "url = 'https://farm4.staticflickr.com/3427/3188200587_fbddbcecbb_z.jpg'\n",
        "img = Image.open(requests.get(url, stream=True).raw)\n",
        "\n",
        "# Define preprocessing pipeline\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "input_tensor = transform(img).unsqueeze(0)\n",
        "\n",
        "# Display the image\n",
        "plt.imshow(img)\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "# Explanation\n",
        "# img: The raw input image loaded from a URL.\n",
        "# transform: Resizes and normalizes the image to match ViT's requirements.\n",
        "# input_tensor: The processed image as a tensor with shape [1, 3, 224, 224]."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "attention_extraction"
      },
      "source": [
        "# 🔍 Extracting Attention Weights\n",
        "\n",
        "To capture attention weights, we modify the forward method of the attention module in the last block. This wrapper saves the attention map internally."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "define_wrapper"
      },
      "outputs": [],
      "source": [
        "from typing import Optional\n",
        "\n",
        "def my_forward_wrapper(attn_obj):\n",
        "    def my_forward(x, attn_mask: Optional[torch.Tensor] = None):\n",
        "        B, N, C = x.shape\n",
        "        qkv = attn_obj.qkv(x).reshape(B, N, 3, attn_obj.num_heads, C // attn_obj.num_heads).permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv.unbind(0)\n",
        "\n",
        "        attn = (q @ k.transpose(-2, -1)) * attn_obj.scale\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = attn_obj.attn_drop(attn)\n",
        "        attn_obj.attn_map = attn\n",
        "        attn_obj.cls_attn_map = attn[:, :, 0, 1:]  # CLS token attention to patches\n",
        "\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
        "        x = attn_obj.proj(x)\n",
        "        x = attn_obj.proj_drop(x)\n",
        "        return x\n",
        "    return my_forward\n",
        "\n",
        "# Apply the wrapper to the last attention module\n",
        "model.blocks[-1].attn.forward = my_forward_wrapper(model.blocks[-1].attn)\n",
        "\n",
        "\n",
        "# Explanation\n",
        "# my_forward_wrapper: Replaces the original forward to compute and store attention maps as attributes.\n",
        "# attn_map: Full attention matrix.\n",
        "# cls_attn_map: Attention from CLS token to image patches."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "run_inference"
      },
      "source": [
        "### Running Inference to Capture Attention\n",
        "\n",
        "Pass the image through the model; the wrapper will save the attention maps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "inference_code"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    output = model(input_tensor)\n",
        "\n",
        "# Explanation\n",
        "# output: Model logits; attention maps are now stored in the attention module."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "process_attention"
      },
      "source": [
        "# 🛠️ Processing Attention Weights\n",
        "\n",
        "Retrieve the saved attention maps, average across heads, and prepare for visualization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "process_attn"
      },
      "outputs": [],
      "source": [
        "# Retrieve attention maps from the module\n",
        "attn_map = model.blocks[-1].attn.attn_map.mean(dim=1).squeeze(0).detach()\n",
        "cls_attn = model.blocks[-1].attn.cls_attn_map.mean(dim=1).squeeze(0).detach()\n",
        "\n",
        "# Reshape CLS attention to patch grid (14x14)\n",
        "grid_size = 14  # 224 / 16\n",
        "cls_attn_map = cls_attn.view(grid_size, grid_size)\n",
        "\n",
        "# Explanation\n",
        "# attn_map: Full attention matrix averaged over heads.\n",
        "# cls_attn_map: CLS token's attention to patches, reshaped to 14x14 grid."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "visualize_attention"
      },
      "source": [
        "# 🎨 Visualizing the Attention Map\n",
        "\n",
        "Overlay the CLS attention map on the original image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "plot_attention"
      },
      "outputs": [],
      "source": [
        "# Resize CLS attention map to image size\n",
        "cls_attn_resized = F.interpolate(cls_attn_map.unsqueeze(0).unsqueeze(0), size=(224, 224), mode='bilinear').squeeze()\n",
        "\n",
        "# Normalize for visualization\n",
        "cls_attn_resized = (cls_attn_resized - cls_attn_resized.min()) / (cls_attn_resized.max() - cls_attn_resized.min())\n",
        "\n",
        "# Convert input tensor to image for overlay\n",
        "img_tensor = input_tensor.squeeze().permute(1, 2, 0).cpu().numpy()\n",
        "img_tensor = (img_tensor * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406]))\n",
        "img_tensor = np.clip(img_tensor, 0, 1)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(img_tensor)\n",
        "plt.title('Original Image')\n",
        "plt.axis('off')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(img_tensor)\n",
        "plt.imshow(cls_attn_resized, cmap='jet', alpha=0.5)\n",
        "plt.title('Attention Map Overlay')\n",
        "plt.axis('off')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# Explanation\n",
        "# cls_attn_resized: Upscaled to 224x224 using bilinear interpolation.\n",
        "# Overlay: Shows attention intensity (red=high, blue=low) on the image."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "interpret_results"
      },
      "source": [
        "# 🧠 Interpreting the Attention Map\n",
        "\n",
        "The attention map highlights regions the model focuses on for classification. High-attention areas (red) are key for the CLS token's representation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "student_task"
      },
      "source": [
        "# # # # # # # # # # # # # # # # # # # # # # # #\n",
        "# 💡 Student Task\n",
        "\n",
        "Use images from the [Microsoft COCO dataset explorer](https://cocodataset.org/#explore).\n",
        "\n",
        "Tasks:\n",
        "1. Load a different image and visualize its attention map.\n",
        "2. Explore the model: Print the number of parameters in the attention module of the first block.\n",
        "3. Access and print the positional embedding shape (`model.pos_embed.shape`).\n",
        "4. Load another ViT variant (e.g., 'vit_small_patch16_224') and compare attention maps.\n",
        "5. Modify the wrapper to capture attention from a different block (e.g., model.blocks[0]) and visualize it.\n",
        "6. Discuss differences in attention across layers or models.\n",
        "\n",
        "Tips:\n",
        "- Use `timm.list_models('vit*')` for variants.\n",
        "- For exploration: Use `sum(p.numel() for p in attn_module.parameters())` for param count."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "student_code"
      },
      "outputs": [],
      "source": [
        "# Starter code\n",
        "print(timm.list_models('vit*'))\n",
        "\n",
        "# Example: Parameter count in attention\n",
        "attn_params = sum(p.numel() for p in model.blocks[0].attn.parameters())\n",
        "print(f\"Attention params in first block: {attn_params:,}\")\n",
        "\n",
        "# Your code here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import timm\n",
        "from PIL import Image\n",
        "import requests\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "# Set up the model\n",
        "model = timm.create_model('resnet50', pretrained=True, num_classes=1000)\n",
        "model.eval()\n",
        "\n",
        "# Download a sample image\n",
        "url = 'http://farm8.staticflickr.com/7012/6597749473_03b2f736ac_z.jpg'\n",
        "img = Image.open(requests.get(url, stream=True).raw)\n",
        "\n",
        "# Define preprocessing pipeline\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "input_tensor = transform(img).unsqueeze(0)\n",
        "\n",
        "# Display the image\n",
        "plt.imshow(img)\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "def get_grad_cam(model, target_layer, input_tensor, target_class=None):\n",
        "    \n",
        "    activations = {}\n",
        "    gradients = {}\n",
        "\n",
        "    def save_activation(module, input, output):\n",
        "        activations['output'] = output.detach()\n",
        "        \n",
        "    def save_gradient(module, grad_in, grad_out):\n",
        "        gradients['output'] = grad_out[0].detach()\n",
        "\n",
        "    # Register the hooks\n",
        "    hook_fwd = target_layer.register_forward_hook(save_activation)\n",
        "    hook_bwd = target_layer.register_backward_hook(save_gradient)\n",
        "\n",
        "    # Perform the forward pass to get the model's output\n",
        "    # This must be done outside `torch.no_grad()` for the backward pass to work\n",
        "    output = model(input_tensor)\n",
        "    \n",
        "    # If a target class isn't specified, use the predicted class\n",
        "    if target_class is None:\n",
        "        target_class = output.argmax()\n",
        "\n",
        "    # Zero gradients and perform backward pass for the target class\n",
        "    model.zero_grad()\n",
        "    one_hot_output = torch.zeros_like(output)\n",
        "    one_hot_output[0][target_class] = 1\n",
        "    output.backward(gradient=one_hot_output, retain_graph=True)\n",
        "\n",
        "    # Remove the hooks to avoid memory leaks\n",
        "    hook_fwd.remove()\n",
        "    hook_bwd.remove()\n",
        "\n",
        "    # Get the feature maps and their gradients\n",
        "    feature_maps = activations['output']\n",
        "    grads = gradients['output']\n",
        "    \n",
        "    # Compute the weights by global average pooling the gradients\n",
        "    weights = torch.mean(grads, dim=(2, 3), keepdim=True)\n",
        "    \n",
        "    # Combine feature maps and weights, then apply ReLU\n",
        "    cam = F.relu(torch.sum(feature_maps * weights, dim=1))\n",
        "    \n",
        "    # Resize the CAM to the size of the original image\n",
        "    cam = F.interpolate(cam.unsqueeze(0), size=(224, 224), mode='bilinear', align_corners=False).squeeze()\n",
        "\n",
        "    return cam\n",
        "\n",
        "# In a ResNet, a good target layer for Grad-CAM is the last convolutional layer.\n",
        "# For a resnet50 from timm, this is typically `model.layer4[-1].conv3`.\n",
        "target_layer = model.layer4[-1].conv3\n",
        "\n",
        "# Run the Grad-CAM generation\n",
        "# The forward pass is now inside the function, outside of any `no_grad` block.\n",
        "grad_cam_map = get_grad_cam(model, target_layer, input_tensor)\n",
        "\n",
        "# Normalize for visualization\n",
        "grad_cam_map = (grad_cam_map - grad_cam_map.min()) / (grad_cam_map.max() - grad_cam_map.min())\n",
        "\n",
        "# Convert input tensor to image for overlay\n",
        "img_tensor = input_tensor.squeeze().permute(1, 2, 0).cpu().numpy()\n",
        "img_tensor = (img_tensor * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406]))\n",
        "img_tensor = np.clip(img_tensor, 0, 1)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(img_tensor)\n",
        "plt.title('Original Image')\n",
        "plt.axis('off')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(img_tensor)\n",
        "plt.imshow(grad_cam_map, cmap='hot', alpha=0.5)\n",
        "plt.title('Grad-CAM Overlay')\n",
        "plt.axis('off')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# Explanation\n",
        "# grad_cam_map: The generated Grad-CAM map, upscaled to the image size.\n",
        "# The corrected code now performs the forward pass outside of a `no_grad`\n",
        "# block, allowing the computation graph to be built and the backward pass to succeed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "author": "Safouane El Ghazouali, Ph.D. in AI, Senior data scientist and researcher at TOELT LLC, Lecturer at HSLU",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
