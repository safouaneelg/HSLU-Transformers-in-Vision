{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SlYqFp-o1xXV"
      },
      "source": [
        "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1wzpTb3vxhXIXhsSPGH13vMeKohVlY1FN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qnsU5KMo1xXW"
      },
      "source": [
        "Author:\n",
        "- **Safouane El Ghazouali**,\n",
        "- Ph.D. in AI,\n",
        "- Senior data scientist and researcher at TOELT LLC,\n",
        "- Lecturer at HSLU\n",
        "\n",
        "# -----  -----  -----  -----  -----  -----  -----  -----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blip_intro"
      },
      "source": [
        "# Hands-on: BLIP-2 for Image Captioning and More\n",
        "\n",
        "Welcome to this comprehensive hands-on notebook on using BLIP-2 for image captioning and related tasks! BLIP-2 (Bootstrapping Language-Image Pre-training 2) from Salesforce is a powerful vision-language model that excels in generating text from images, including captioning and visual question answering (VQA).\n",
        "\n",
        "![BLIP-2 Example](https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg)\n",
        "\n",
        "### Why Use BLIP-2?\n",
        "- **Multimodal Capabilities**: Generates captions, answers questions, and supports chat-like interactions.\n",
        "- **Efficiency**: Uses frozen image encoders and LLMs with a lightweight Q-Former bridge.\n",
        "- **Zero-Shot**: Performs well on unseen images without fine-tuning.\n",
        "- **Hugging Face Integration**: Easy to load and use via Transformers.\n",
        "\n",
        "### What You'll Learn\n",
        "- Loading BLIP-2 from Hugging Face.\n",
        "- Basic image captioning on single images from URLs.\n",
        "- Prompted captioning and visual question answering (VQA).\n",
        "- Batch processing multiple images.\n",
        "- Handling different model precisions (e.g., float16 for GPU).\n",
        "- Exploring outputs and use cases."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "env_setup"
      },
      "source": [
        "# üß∞ Environment Setup\n",
        "\n",
        "Install Transformers and dependencies for image handling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_deps"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers requests pillow matplotlib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imports"
      },
      "source": [
        "### Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "import_libs"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import requests\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "from transformers import Blip2Processor, Blip2ForConditionalGeneration\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f'Using device: {device}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blip_explain"
      },
      "source": [
        "# üìö Understanding BLIP-2\n",
        "\n",
        "BLIP-2 bridges vision and language using a Q-Former to connect frozen image encoders (e.g., ViT) and LLMs (e.g., OPT). It supports:\n",
        "- **Captioning**: Generate descriptions without prompts.\n",
        "- **Prompted Captioning**: Condition on text for guided generation.\n",
        "- **VQA**: Answer questions about images.\n",
        "- **Chat-like**: Feed prompts for conversational outputs.\n",
        "\n",
        "Reference: [Hugging Face BLIP-2](https://huggingface.co/Salesforce/blip2-opt-2.7b)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "load_model"
      },
      "source": [
        "# üì¶ Loading the Model\n",
        "\n",
        "Load the processor and model. Use float16 on GPU for efficiency."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "load_blip"
      },
      "outputs": [],
      "source": [
        "processor = Blip2Processor.from_pretrained('Salesforce/blip2-opt-2.7b')\n",
        "model = Blip2ForConditionalGeneration.from_pretrained(\n",
        "    'Salesforce/blip2-opt-2.7b',\n",
        "    torch_dtype=torch.float16 if device == 'cuda' else torch.float32\n",
        ").to(device)\n",
        "print('BLIP-2 loaded!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "basic_captioning"
      },
      "source": [
        "# üñºÔ∏è Basic Image Captioning\n",
        "\n",
        "Generate captions without prompts on a single image from URL."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "basic_code"
      },
      "outputs": [],
      "source": [
        "# Sample image URL\n",
        "img_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\n",
        "response = requests.get(img_url)\n",
        "image = Image.open(BytesIO(response.content)).convert('RGB')\n",
        "\n",
        "# Display\n",
        "plt.imshow(image)\n",
        "plt.title('Sample Image')\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "# Captioning (no prompt)\n",
        "inputs = processor(image, return_tensors='pt').to(device, dtype=model.dtype)\n",
        "outputs = model.generate(**inputs, max_new_tokens=50)\n",
        "caption = processor.decode(outputs[0], skip_special_tokens=True).strip()\n",
        "print(f'Generated Caption: {caption}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prompted_captioning"
      },
      "source": [
        "# ‚úçÔ∏è Prompted Captioning\n",
        "\n",
        "Guide generation with a text prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "prompted_code"
      },
      "outputs": [],
      "source": [
        "# Prompted\n",
        "prompt = 'A woman playing with'\n",
        "inputs_prompted = processor(image, text=prompt, return_tensors='pt').to(device, dtype=model.dtype)\n",
        "outputs_prompted = model.generate(**inputs_prompted, max_new_tokens=50)\n",
        "caption_prompted = processor.decode(outputs_prompted[0], skip_special_tokens=True).strip()\n",
        "print(f'Prompted Caption: {caption_prompted}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vqa_usecase"
      },
      "source": [
        "# ‚ùì Visual Question Answering (VQA)\n",
        "\n",
        "Ask questions about the image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vqa_code"
      },
      "outputs": [],
      "source": [
        "# VQA\n",
        "question = 'Question: where is the person and her dog playing? Answer:'\n",
        "inputs_vqa = processor(image, text=question, return_tensors='pt').to(device, dtype=model.dtype)\n",
        "outputs_vqa = model.generate(**inputs_vqa, max_new_tokens=50)\n",
        "answer = processor.decode(outputs_vqa[0], skip_special_tokens=True).strip()\n",
        "print(f'VQA Answer: {answer}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "batch_processing"
      },
      "source": [
        "# üì∏ Batch Processing Multiple Images\n",
        "\n",
        "Caption multiple images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "batch_code"
      },
      "outputs": [],
      "source": [
        "# Multiple URLs\n",
        "img_urls = [\n",
        "    'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg',\n",
        "    'http://images.cocodataset.org/val2017/000000039769.jpg'  # Another example\n",
        "]\n",
        "\n",
        "for url in img_urls:\n",
        "    response = requests.get(url)\n",
        "    img_batch = Image.open(BytesIO(response.content)).convert('RGB')\n",
        "    inputs_batch = processor(img_batch, return_tensors='pt').to(device, dtype=model.dtype)\n",
        "    outputs_batch = model.generate(**inputs_batch, max_new_tokens=50)\n",
        "    caption_batch = processor.decode(outputs_batch[0], skip_special_tokens=True).strip()\n",
        "    print(f'Caption for {url}: {caption_batch}')\n",
        "\n",
        "    # Display\n",
        "    plt.imshow(img_batch)\n",
        "    plt.title(caption_batch)\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chat_usecase"
      },
      "source": [
        "# üí¨ Chat-like Interaction\n",
        "\n",
        "Simulate conversation by chaining prompts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "chat_code"
      },
      "outputs": [],
      "source": [
        "# Chat example\n",
        "\n",
        "img_path = \"https://cdn.shopify.com/s/files/1/0817/1687/1489/files/A_man_playing_fetch_with_a_black_dog_in_a_grassy_field_pointing_ahead_while_the_dog_happily_runs_towards_a_blue_and_orange_flying_disc_on_the_ground.png?v=1728597464\"\n",
        "\n",
        "image = Image.open(BytesIO(requests.get(img_path).content)).convert('RGB')\n",
        "plt.imshow(image)\n",
        "plt.title('Sample Image')\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "chat_prompt = 'User: Describe the scene.\\nAssistant: A person is playing frisbee with her dog on the beach.\\nUser: What color is the frisbee? Answer:'\n",
        "inputs_chat = processor(image, text=chat_prompt, return_tensors='pt').to(device, dtype=model.dtype)\n",
        "outputs_chat = model.generate(**inputs_chat, max_new_tokens=50)\n",
        "response_chat = processor.decode(outputs_chat[0], skip_special_tokens=True).strip()\n",
        "print(f'Chat Response: {response_chat}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cEa-d10B336_"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}