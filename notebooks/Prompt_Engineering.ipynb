{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1dTHQAnkWkbD68_iPiNT0A2Qs9zBEu2Es)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: \n",
    "- **Safouane El Ghazouali**, \n",
    "- Ph.D. in AI, \n",
    "- Senior data scientist and researcher at TOELT LLC,\n",
    "- Lecturer at HSLU\n",
    "\n",
    "# -----  -----  -----  -----  -----  -----  -----  -----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "prompt_eng_intro"
   },
   "source": [
    "# Hands-on: Prompt Engineering for CLIP and GroundingDINO\n",
    "\n",
    "Welcome to this comprehensive hands-on notebook on prompt engineering! We'll explore best practices to optimize prompts for zero-shot classification with CLIP and zero-shot detection with GroundingDINO, enhancing model performance without additional training.\n",
    "\n",
    "![Prompt Engineering](https://www.promptingguide.ai/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fprompt.3c3c4c4f.png&w=1920&q=75)\n",
    "\n",
    "### Why Prompt Engineering?\n",
    "- **Optimization**: Well-crafted prompts can significantly boost accuracy in zero-shot tasks.\n",
    "- **Flexibility**: Adapt models to specific domains or nuances via text alone.\n",
    "- **Efficiency**: No need for retraining; iterate on prompts for quick improvements.\n",
    "- **Best Practices**: For CLIP: Use ensembles, descriptive templates. For GroundingDINO: Lowercase, dot-separated phrases, thresholds.\n",
    "\n",
    "### What You'll Learn\n",
    "- Crafting effective prompts for CLIP classification.\n",
    "- Optimizing detection prompts for GroundingDINO.\n",
    "- Testing variations on images from URLs.\n",
    "- Analyzing impacts on results.\n",
    "- Applying to small datasets like CIFAR10/COCO samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "env_setup"
   },
   "source": [
    "# üß∞ Environment Setup\n",
    "\n",
    "Install required libraries for CLIP, GroundingDINO, and utilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_deps"
   },
   "outputs": [],
   "source": [
    "!pip install -q git+https://github.com/openai/CLIP.git transformers requests pillow opencv-python matplotlib numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "imports"
   },
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "import_libs"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import clip\n",
    "from transformers import AutoProcessor, AutoModelForZeroShotObjectDetection\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "clip_section"
   },
   "source": [
    "# üìù Prompt Engineering for CLIP (Classification)\n",
    "\n",
    "Best Practices:\n",
    "- Use descriptive templates like \"a photo of a {class}\".\n",
    "- Ensemble multiple prompts for robustness (e.g., average similarities).\n",
    "- Add context: \"a low-quality photo of a {class}\" for domain adaptation.\n",
    "- Avoid ambiguity; be specific to reduce misclassifications.\n",
    "\n",
    "Load CLIP model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load_clip"
   },
   "outputs": [],
   "source": [
    "clip_model, clip_preprocess = clip.load('ViT-B/32', device=device)\n",
    "print('CLIP loaded!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "clip_single_image"
   },
   "source": [
    "### Single Image with Basic vs. Engineered Prompts\n",
    "\n",
    "Test prompt variations on an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "clip_prompt_test"
   },
   "outputs": [],
   "source": [
    "# Image URL (e.g., a cat)\n",
    "url = 'https://images.unsplash.com/photo-1541963463532-daf8c885265c'\n",
    "response = requests.get(url)\n",
    "img = Image.open(BytesIO(response.content))\n",
    "img_preprocessed = clip_preprocess(img).unsqueeze(0).to(device)\n",
    "\n",
    "plt.imshow(img)\n",
    "plt.title('Sample Image')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# Classes\n",
    "classes = ['cat', 'dog', 'car']\n",
    "\n",
    "# Basic prompt\n",
    "basic_prompts = [f'{c}' for c in classes]\n",
    "text_basic = clip.tokenize(basic_prompts).to(device)\n",
    "\n",
    "# Engineered prompt\n",
    "eng_prompts = [f'a high-quality photo of a {c} in natural light' for c in classes]\n",
    "text_eng = clip.tokenize(eng_prompts).to(device)\n",
    "\n",
    "# Inference function\n",
    "def clip_predict(text_inputs):\n",
    "    with torch.no_grad():\n",
    "        image_features = clip_model.encode_image(img_preprocessed)\n",
    "        text_features = clip_model.encode_text(text_inputs)\n",
    "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "        similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "    return similarity[0].cpu().numpy()\n",
    "\n",
    "# Results\n",
    "basic_probs = clip_predict(text_basic)\n",
    "eng_probs = clip_predict(text_eng)\n",
    "\n",
    "print('Basic Prompt Probabilities:')\n",
    "for c, p in zip(classes, basic_probs):\n",
    "    print(f'{c}: {p*100:.2f}%')\n",
    "\n",
    "print('\\nEngineered Prompt Probabilities:')\n",
    "for c, p in zip(classes, eng_probs):\n",
    "    print(f'{c}: {p*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "clip_ensemble"
   },
   "source": [
    "### Prompt Ensemble\n",
    "\n",
    "Average multiple templates for better robustness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "clip_ensemble_code"
   },
   "outputs": [],
   "source": [
    "templates = [\n",
    "    'a photo of a {}',\n",
    "    'an image of {}',\n",
    "    'a picture of the {}'\n",
    "]\n",
    "\n",
    "ensemble_probs = np.zeros(len(classes))\n",
    "for temp in templates:\n",
    "    text_ens = clip.tokenize([temp.format(c) for c in classes]).to(device)\n",
    "    ensemble_probs += clip_predict(text_ens)\n",
    "\n",
    "ensemble_probs /= len(templates)\n",
    "\n",
    "print('Ensemble Probabilities:')\n",
    "for c, p in zip(classes, ensemble_probs):\n",
    "    print(f'{c}: {p*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "groundingdino_section"
   },
   "source": [
    "# üìù Prompt Engineering for GroundingDINO (Detection)\n",
    "\n",
    "Best Practices:\n",
    "- Lowercase phrases, separate with '.', end each with '.' (e.g., \"cat. remote.\").\n",
    "- Be specific: \"orange cat. tv remote.\" for better matching.\n",
    "- Adjust thresholds based on prompt complexity.\n",
    "- Use for open-set: Detect novel objects via descriptive text.\n",
    "\n",
    "Load GroundingDINO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load_dino"
   },
   "outputs": [],
   "source": [
    "model_id = 'IDEA-Research/grounding-dino-base'\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "model = AutoModelForZeroShotObjectDetection.from_pretrained(model_id).to(device)\n",
    "print('GroundingDINO loaded!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dino_single_image"
   },
   "source": [
    "### Single Image with Basic vs. Engineered Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dino_prompt_test"
   },
   "outputs": [],
   "source": [
    "# Image URL\n",
    "url = 'http://images.cocodataset.org/val2017/000000039769.jpg'  # Cats and remotes\n",
    "response = requests.get(url)\n",
    "img = Image.open(BytesIO(response.content))\n",
    "\n",
    "plt.imshow(img)\n",
    "plt.title('Sample Image')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# Basic prompt\n",
    "basic_prompt = 'cat. remote.'\n",
    "\n",
    "# Engineered prompt\n",
    "eng_prompt = 'orange cat. brown cat. black remote. white remote.'\n",
    "\n",
    "# Function to detect and visualize\n",
    "def dino_detect(prompt, box_th=0.35, text_th=0.25):\n",
    "    inputs = processor(images=img, text=prompt, return_tensors='pt').to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    results = processor.post_process_grounded_object_detection(\n",
    "        outputs, inputs.input_ids, box_threshold=box_th, text_threshold=text_th, target_sizes=[img.size[::-1]]\n",
    "    )[0]\n",
    "    \n",
    "    img_cv = cv2.cvtColor(np.array(img), cv2.COLOR_RGB2BGR)\n",
    "    for box, score, label in zip(results['boxes'], results['scores'], results['labels']):\n",
    "        box = [int(c) for c in box]\n",
    "        cv2.rectangle(img_cv, (box[0], box[1]), (box[2], box[3]), (0, 255, 0), 2)\n",
    "        cv2.putText(img_cv, f'{label}: {score:.2f}', (box[0], box[1]-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "    \n",
    "    plt.imshow(cv2.cvtColor(img_cv, cv2.COLOR_BGR2RGB))\n",
    "    plt.title(f'Detections with Prompt: {prompt}')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Basic\n",
    "print('Basic Prompt Results:')\n",
    "basic_results = dino_detect(basic_prompt)\n",
    "print(basic_results)\n",
    "\n",
    "# Engineered\n",
    "print('\\nEngineered Prompt Results:')\n",
    "eng_results = dino_detect(eng_prompt)\n",
    "print(eng_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dino_thresholds"
   },
   "source": [
    "### Adjusting Thresholds with Prompts\n",
    "\n",
    "Lower thresholds for more detections, higher for precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dino_thresh_code"
   },
   "outputs": [],
   "source": [
    "print('Low Threshold Results:')\n",
    "low_results = dino_detect(eng_prompt, box_th=0.2, text_th=0.15)\n",
    "print(low_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "interpret_results"
   },
   "source": [
    "# üß† Interpreting Results\n",
    "\n",
    "Engineered prompts yield more precise matches. Ensembles in CLIP reduce variance. For DINO, specific descriptors and '.' separation improve grounding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "student_task"
   },
   "source": [
    "# üí° Student Task\n",
    "\n",
    "1. Test custom prompts on a new image URL for CLIP.\n",
    "2. Add more templates to CLIP ensemble.\n",
    "3. For DINO, try prompts with colors/attributes.\n",
    "4. Compare basic vs. engineered on CIFAR10 sample.\n",
    "5. Adjust thresholds and observe trade-offs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "student_code"
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
