{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8Sk4tmq1TOf"
      },
      "source": [
        "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1T6kUzlBPIi4V8RAl7m95LZMLUkJsnWhU)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXs2017r1TOg"
      },
      "source": [
        "Author:\n",
        "- **Safouane El Ghazouali**,\n",
        "- Ph.D. in AI,\n",
        "- Senior data scientist and researcher at TOELT LLC,\n",
        "- Lecturer at HSLU\n",
        "\n",
        "# -----  -----  -----  -----  -----  -----  -----  -----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clip_intro"
      },
      "source": [
        "# Hands-on: CLIP for Zero-Shot Image Classification\n",
        "\n",
        "Welcome to this comprehensive hands-on notebook on using CLIP (Contrastive Language-Image Pre-training) for zero-shot image classification! CLIP, developed by OpenAI, enables models to classify images without task-specific training by leveraging natural language prompts.\n",
        "\n",
        "![CLIP Example](https://cdn.prod.website-files.com/651c34ac817aad4a2e62ec1b/653694d75b5a654627f55a91_image%20(7).png)\n",
        "\n",
        "### Why Use CLIP?\n",
        "- **Zero-Shot Capability**: Classify images using arbitrary text labels without retraining.\n",
        "- **Flexibility**: Handles diverse tasks like classification, retrieval, and more via prompts.\n",
        "- **Robustness**: Trained on 400M image-text pairs, generalizes well to new domains.\n",
        "- **Open-Source Alternatives**: Projects like OpenCLIP extend CLIP with more models and datasets.\n",
        "\n",
        "### What You'll Learn\n",
        "- Loading and using OpenAI's CLIP and OpenCLIP models.\n",
        "- Performing zero-shot classification on single images from URLs.\n",
        "- Batch processing multiple images with custom prompts.\n",
        "- Evaluating zero-shot performance on a small dataset like CIFAR10.\n",
        "- Prompt engineering, ensembles, top-k metrics, and visualization.\n",
        "- Comparing models from different repositories."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "env_setup"
      },
      "source": [
        "# ðŸ§° Environment Setup\n",
        "\n",
        "Install CLIP, OpenCLIP, and dependencies for data handling and visualization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_deps"
      },
      "outputs": [],
      "source": [
        "!pip install -q git+https://github.com/openai/CLIP.git requests pillow\n",
        "!pip install -q open_clip_torch torchvision matplotlib scikit-learn tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imports"
      },
      "source": [
        "### Import Libraries\n",
        "\n",
        "Import modules for model loading, image processing, and analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "import_libs"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import clip\n",
        "import open_clip\n",
        "import requests\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from tqdm import tqdm\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f'Using device: {device}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clip_explain"
      },
      "source": [
        "# ðŸ“š Understanding CLIP and Zero-Shot Classification\n",
        "\n",
        "CLIP learns visual concepts from natural language supervision. For zero-shot:\n",
        "1. Encode image and text prompts into embeddings.\n",
        "2. Compute cosine similarities.\n",
        "3. Select the highest similarity as the prediction.\n",
        "\n",
        "We'll start with single images, then batches, and evaluate on CIFAR10.\n",
        "\n",
        "Reference: [OpenAI CLIP GitHub](https://github.com/openai/CLIP) - Trained on 400M pairs.\n",
        "[OpenCLIP GitHub](https://github.com/mlfoundations/open_clip) - Open-source with LAION models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "load_clip"
      },
      "source": [
        "# ðŸ“¦ Loading OpenAI CLIP Model\n",
        "\n",
        "Load a pre-trained CLIP model (ViT-B/32 for balance)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "clip_model"
      },
      "outputs": [],
      "source": [
        "clip_model, clip_preprocess = clip.load('ViT-B/32', device=device)\n",
        "print('OpenAI CLIP model loaded!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "single_image_inference"
      },
      "source": [
        "# ðŸ–¼ï¸ Zero-Shot Classification on a Single Image\n",
        "\n",
        "Download an image from a URL, define text prompts, and compute predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "single_image_code"
      },
      "outputs": [],
      "source": [
        "# Example image URL (a bird)\n",
        "image_url = 'http://farm6.staticflickr.com/5517/9349775899_6a2d58ab9a_z.jpg'\n",
        "\n",
        "# Download and preprocess\n",
        "response = requests.get(image_url)\n",
        "img = Image.open(BytesIO(response.content))\n",
        "img_preprocessed = clip_preprocess(img).unsqueeze(0).to(device)\n",
        "\n",
        "# Display image\n",
        "plt.imshow(img)\n",
        "plt.title('Sample Image')\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "# Text prompts\n",
        "prompts = ['a cat', 'a dog', 'a car', 'a bird']\n",
        "text_inputs = clip.tokenize([f'a photo of {p}' for p in prompts]).to(device)\n",
        "\n",
        "# Inference\n",
        "with torch.no_grad():\n",
        "    image_features = clip_model.encode_image(img_preprocessed)\n",
        "    text_features = clip_model.encode_text(text_inputs)\n",
        "\n",
        "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "    similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
        "    values, indices = similarity[0].topk(len(prompts))\n",
        "\n",
        "# Results\n",
        "print('Top Predictions:')\n",
        "for value, index in zip(values, indices):\n",
        "    print(f'{prompts[index]:>16s}: {100 * value.item():.2f}%')\n",
        "\n",
        "# Explanation\n",
        "# - Image downloaded via URL and preprocessed.\n",
        "# - Prompts tokenized and encoded.\n",
        "# - Similarities computed; highest is the prediction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "batch_images"
      },
      "source": [
        "# ðŸ“¸ Batch Processing Multiple Images\n",
        "\n",
        "Process a list of image URLs with shared prompts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "batch_code"
      },
      "outputs": [],
      "source": [
        "# List of image URLs\n",
        "image_urls = [\n",
        "    'http://farm3.staticflickr.com/2607/4152809797_e86483de70_z.jpg',  # Cat\n",
        "    'http://farm8.staticflickr.com/7276/7051382257_b45a38c0ec_z.jpg',  # Dog\n",
        "    'http://farm6.staticflickr.com/5306/5653379279_49e1b67bc2_z.jpg'   # Car\n",
        "]\n",
        "\n",
        "# Prompts\n",
        "prompts = ['a cat', 'a dog', 'a car', 'a bird']\n",
        "text_inputs = clip.tokenize([f'a photo of {p}' for p in prompts]).to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    text_features = clip_model.encode_text(text_inputs)\n",
        "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "# Process images\n",
        "fig, axs = plt.subplots(1, len(image_urls), figsize=(15, 5))\n",
        "for i, url in enumerate(image_urls):\n",
        "    response = requests.get(url)\n",
        "    img = Image.open(BytesIO(response.content))\n",
        "    img_preprocessed = clip_preprocess(img).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        image_features = clip_model.encode_image(img_preprocessed)\n",
        "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "        similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
        "        pred_idx = similarity.argmax().item()\n",
        "\n",
        "    axs[i].imshow(img)\n",
        "    axs[i].set_title(f'Pred: {prompts[pred_idx]}')\n",
        "    axs[i].axis('off')\n",
        "plt.show()\n",
        "\n",
        "# Explanation\n",
        "# - Multiple images processed in a loop.\n",
        "# - Text features computed once for efficiency.\n",
        "# - Predictions visualized with images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cifar10_section"
      },
      "source": [
        "# ðŸ“Š Evaluating on a Small Dataset (CIFAR10 Subset)\n",
        "\n",
        "Now, apply zero-shot to a small subset of CIFAR10 for systematic evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cifar10_load"
      },
      "outputs": [],
      "source": [
        "# Define classes\n",
        "cifar_classes = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "\n",
        "# Load CIFAR10 test set\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Small subset (100 images)\n",
        "subset_indices = list(range(100))\n",
        "subset_dataset = Subset(test_dataset, subset_indices)\n",
        "test_loader = DataLoader(subset_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "print(f'Loaded CIFAR10 subset with {len(subset_dataset)} images.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prepare_prompts"
      },
      "source": [
        "# âœï¸ Preparing Text Prompts\n",
        "\n",
        "Use templates for CIFAR10 classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "prompts"
      },
      "outputs": [],
      "source": [
        "prompt_template = 'a photo of a {}'\n",
        "text_inputs = clip.tokenize([prompt_template.format(c) for c in cifar_classes]).to(device)\n",
        "\n",
        "ensemble_templates = [\n",
        "    'a photo of a {}',\n",
        "    'an image of {}',\n",
        "    'a picture of the {}'\n",
        "]\n",
        "print('Prompts ready!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zero_shot_inference"
      },
      "source": [
        "# ðŸ” Zero-Shot on CIFAR10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "inference_clip"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    text_features = clip_model.encode_text(text_inputs)\n",
        "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "predictions = []\n",
        "labels_list = []\n",
        "for images, labels in tqdm(test_loader):\n",
        "    images = torch.stack([clip_preprocess(transforms.ToPILImage()(img)) for img in images]).to(device)\n",
        "    image_features = clip_model.encode_image(images)\n",
        "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "    similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
        "    preds = similarity.argmax(dim=1).cpu().numpy()\n",
        "    predictions.extend(preds)\n",
        "    labels_list.extend(labels.numpy())\n",
        "\n",
        "accuracy = np.mean(np.array(predictions) == np.array(labels_list)) * 100\n",
        "print(f'Accuracy: {accuracy:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "openclip_section"
      },
      "source": [
        "# ðŸ”„ OpenCLIP Models\n",
        "\n",
        "Load and test an OpenCLIP model for comparison."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "openclip_load"
      },
      "outputs": [],
      "source": [
        "openclip_model, _, openclip_preprocess = open_clip.create_model_and_transforms('ViT-B-32', pretrained='laion2b_s34b_b79k')\n",
        "openclip_model.to(device).eval()\n",
        "openclip_tokenizer = open_clip.get_tokenizer('ViT-B-32')\n",
        "\n",
        "image_url = 'http://farm6.staticflickr.com/5517/9349775899_6a2d58ab9a_z.jpg'\n",
        "\n",
        "# Download and preprocess\n",
        "response = requests.get(image_url)\n",
        "img = Image.open(BytesIO(response.content))\n",
        "img_preprocessed = openclip_preprocess(img).unsqueeze(0).to(device)\n",
        "\n",
        "# Display image\n",
        "plt.imshow(img)\n",
        "plt.title('Sample Image')\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "# Text prompts\n",
        "prompts = ['a cat', 'a dog', 'a car', 'a bird']\n",
        "text_inputs = openclip_tokenizer([f'a photo of {p}' for p in prompts]).to(device)\n",
        "\n",
        "# Inference\n",
        "with torch.no_grad():\n",
        "    image_features = openclip_model.encode_image(img_preprocessed)\n",
        "    text_features = openclip_model.encode_text(text_inputs)\n",
        "\n",
        "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "    similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
        "    values, indices = similarity[0].topk(len(prompts))\n",
        "\n",
        "# Results\n",
        "print('Top Predictions:')\n",
        "for value, index in zip(values, indices):\n",
        "    print(f'{prompts[index]:>16s}: {100 * value.item():.2f}%')\n",
        "\n",
        "# Explanation\n",
        "# - Image downloaded via URL and preprocessed.\n",
        "# - Prompts tokenized and encoded.\n",
        "# - Similarities computed; highest is the prediction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "student_task"
      },
      "source": [
        "# ðŸ’¡ Student Task\n",
        "\n",
        "1. Classify a custom image URL with your own prompts.\n",
        "2. Test batch on more challenging image from COCO dataset.\n",
        "3. Evaluate full CIFAR10.\n",
        "4. Check out OpenCLIP github repo and try other models."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BKM31BIm4tdy"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}