{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1omtfPT5WueHxXk_sqO452eV0-Ex76K1U)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: \n",
    "- **Safouane El Ghazouali**, \n",
    "- Ph.D. in AI, \n",
    "- Senior data scientist and researcher at TOELT LLC,\n",
    "- Lecturer at HSLU\n",
    "\n",
    "# -----  -----  -----  -----  -----  -----  -----  -----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "groundingdino_intro"
   },
   "source": [
    "# Hands-on: GroundingDINO for Zero-Shot Object Detection\n",
    "\n",
    "Welcome to this comprehensive hands-on notebook on using GroundingDINO for zero-shot object detection! GroundingDINO combines DINO and GLIP to detect objects using free-form text prompts, enabling open-vocabulary detection without training.\n",
    "\n",
    "![GroundingDINO Example](https://huggingface.co/IDEA-Research/grounding-dino-base/resolve/main/demo_image.jpg)\n",
    "\n",
    "### Why Use GroundingDINO?\n",
    "- **Zero-Shot Detection**: Detect arbitrary objects via text prompts like \"red car. person in blue shirt\".\n",
    "- **Open-Vocabulary**: Handles unseen classes without labeled data.\n",
    "- **Flexibility**: Useful for custom detection in robotics, surveillance, etc.\n",
    "- **Hugging Face Integration**: Easy loading via Transformers library.\n",
    "\n",
    "### What You'll Learn\n",
    "- Loading the GroundingDINO model from Hugging Face.\n",
    "- Performing inference on single images from URLs.\n",
    "- Processing outputs: bounding boxes, logits, phrases.\n",
    "- Visualizing detections with boxes and labels.\n",
    "- Adjusting thresholds and using multiple prompts.\n",
    "- Batch processing and analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "env_setup"
   },
   "source": [
    "# üß∞ Environment Setup\n",
    "\n",
    "Install Transformers and dependencies for image processing and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_deps"
   },
   "outputs": [],
   "source": [
    "!pip install -q transformers requests pillow opencv-python matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "imports"
   },
   "source": [
    "### Import Libraries\n",
    "\n",
    "Import modules for model loading, image handling, and drawing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "import_libs"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import torch\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from transformers import AutoProcessor, AutoModelForZeroShotObjectDetection\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "groundingdino_explain"
   },
   "source": [
    "# üìö Understanding GroundingDINO\n",
    "\n",
    "GroundingDINO detects objects by grounding text descriptions to image regions. Key:\n",
    "- Prompts: Lowercase phrases ending with '.' (e.g., \"cat. remote control.\").\n",
    "- Outputs: Bounding boxes, confidence logits, matched phrases.\n",
    "- Thresholds: Box (0.35+) and text (0.25+) for filtering.\n",
    "\n",
    "Reference: [Hugging Face Model Card](https://huggingface.co/IDEA-Research/grounding-dino-base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "load_model"
   },
   "source": [
    "# üì¶ Loading the Model\n",
    "\n",
    "Load the processor and model from Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load_groundingdino"
   },
   "outputs": [],
   "source": [
    "model_id = \"IDEA-Research/grounding-dino-base\"\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "model = AutoModelForZeroShotObjectDetection.from_pretrained(model_id).to(device)\n",
    "print('GroundingDINO loaded!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "single_image_inference"
   },
   "source": [
    "# üñºÔ∏è Zero-Shot Detection on a Single Image\n",
    "\n",
    "Download an image via URL, prepare prompt, and run detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "single_inference"
   },
   "outputs": [],
   "source": [
    "# Sample image URL (COCO example with cats and remotes)\n",
    "image_url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "response = requests.get(image_url)\n",
    "image = Image.open(BytesIO(response.content))\n",
    "\n",
    "# Display original image\n",
    "plt.imshow(image)\n",
    "plt.title('Sample Image')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# Text prompt (lowercase, end with .)\n",
    "text = \"a cat. a remote control.\"\n",
    "\n",
    "# Prepare inputs\n",
    "inputs = processor(images=image, text=text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Inference\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Post-process\n",
    "results = processor.post_process_grounded_object_detection(\n",
    "    outputs,\n",
    "    inputs.input_ids,\n",
    "    box_threshold=0.35,\n",
    "    text_threshold=0.25,\n",
    "    target_sizes=[image.size[::-1]]\n",
    ")[0]  # Take first (only) result\n",
    "\n",
    "print('Detection Results:')\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "visualize_detections"
   },
   "source": [
    "# üé® Visualizing Detections\n",
    "\n",
    "Draw bounding boxes and labels on the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vis_code"
   },
   "outputs": [],
   "source": [
    "# Convert PIL to OpenCV\n",
    "img_cv = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)\n",
    "\n",
    "# Draw boxes\n",
    "for box, score, label in zip(results['boxes'], results['scores'], results['labels']):\n",
    "    box = [int(coord) for coord in box.tolist()]\n",
    "    cv2.rectangle(img_cv, (box[0], box[1]), (box[2], box[3]), (0, 255, 0), 2)\n",
    "    text = f'{label}: {score:.2f}'\n",
    "    cv2.putText(img_cv, text, (box[0], box[1] - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "# Display\n",
    "plt.imshow(cv2.cvtColor(img_cv, cv2.COLOR_BGR2RGB))\n",
    "plt.title('Detected Objects')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# Explanation\n",
    "# - boxes: [x_min, y_min, x_max, y_max]\n",
    "# - scores: Confidence logits\n",
    "# - labels: Matched phrases from prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "adjust_thresholds"
   },
   "source": [
    "# ‚öôÔ∏è Adjusting Thresholds\n",
    "\n",
    "Tune box_threshold and text_threshold to filter detections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "threshold_code"
   },
   "outputs": [],
   "source": [
    "# Re-post-process with higher thresholds\n",
    "results_high = processor.post_process_grounded_object_detection(\n",
    "    outputs,\n",
    "    inputs.input_ids,\n",
    "    box_threshold=0.5,\n",
    "    text_threshold=0.4,\n",
    "    target_sizes=[image.size[::-1]]\n",
    ")[0]\n",
    "\n",
    "print('High Threshold Results:')\n",
    "print(results_high)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "multiple_prompts"
   },
   "source": [
    "# üîÑ Using Multiple Prompts\n",
    "\n",
    "Test different prompts on the same image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "multi_prompt_code"
   },
   "outputs": [],
   "source": [
    "text2 = \"furniture. animal. electronic device.\"\n",
    "inputs2 = processor(images=image, text=text2, return_tensors=\"pt\").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs2 = model(**inputs2)\n",
    "\n",
    "results2 = processor.post_process_grounded_object_detection(\n",
    "    outputs2,\n",
    "    inputs2.input_ids,\n",
    "    box_threshold=0.35,\n",
    "    text_threshold=0.25,\n",
    "    target_sizes=[image.size[::-1]]\n",
    ")[0]\n",
    "\n",
    "print('Alternative Prompt Results:')\n",
    "print(results2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "batch_inference"
   },
   "source": [
    "# üì∏ Batch Processing Multiple Images\n",
    "\n",
    "Process a list of image URLs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "batch_code"
   },
   "outputs": [],
   "source": [
    "# List of URLs\n",
    "image_urls = [\n",
    "    \"http://images.cocodataset.org/val2017/000000039769.jpg\",\n",
    "    \"http://images.cocodataset.org/val2017/000000281759.jpg\"  # Another COCO image\n",
    "]\n",
    "\n",
    "text = \"person. car. dog.\"\n",
    "\n",
    "for url in image_urls:\n",
    "    response = requests.get(url)\n",
    "    img = Image.open(BytesIO(response.content))\n",
    "    inputs_batch = processor(images=img, text=text, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs_batch = model(**inputs_batch)\n",
    "    \n",
    "    results_batch = processor.post_process_grounded_object_detection(\n",
    "        outputs_batch,\n",
    "        inputs_batch.input_ids,\n",
    "        box_threshold=0.35,\n",
    "        text_threshold=0.25,\n",
    "        target_sizes=[img.size[::-1]]\n",
    "    )[0]\n",
    "    \n",
    "    print(f'Results for {url}:')\n",
    "    print(results_batch)\n",
    "\n",
    "    # Visualize (similar to above)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "interpret_results"
   },
   "source": [
    "# üß† Interpreting Results\n",
    "\n",
    "Outputs include filtered boxes, scores (logits), and phrases matched from the prompt. Higher thresholds reduce false positives but may miss detections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "student_task"
   },
   "source": [
    "# üí° Student Task\n",
    "\n",
    "1. Test on a custom image URL with your own prompt.\n",
    "2. Experiment with thresholds to balance precision/recall.\n",
    "3. Use complex prompts (e.g., \"red apple. green bottle.\").\n",
    "4. Batch process 3+ URLs and visualize.\n",
    "5. Compare detections across different prompts on the same image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "student_code"
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
