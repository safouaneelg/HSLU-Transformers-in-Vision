{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "426bcb0f",
   "metadata": {},
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1FK1NBPGBwnRs3tJlCGmPEg26byt_kIgz)\n",
    "\n",
    "Author:\n",
    "- **Safouane El Ghazouali**,\n",
    "- Ph.D. in AI,\n",
    "- Senior data scientist and researcher at TOELT LLC,\n",
    "- Lecturer at HSLU\n",
    "\n",
    "# -----  -----  -----  -----  -----  -----  -----  -----\n",
    "\n",
    "# üîç Exploring Models Inferencing using Ultralytics Library\n",
    "\n",
    "Welcome to this hands-on notebook on exploring pre-trained object detection models using the Ultralytics library. This library provides convenient access to several state-of-the-art models like YOLOv5, YOLOv8, YOLOv10, and RT-DETR.\n",
    "\n",
    "![YOLO Example](https://cdn.prod.website-files.com/680a070c3b99253410dd3df5/680a070c3b99253410dd4791_67ed5670d7ecbda0527fe8b3_66f680814693dd5c3b60dfcb_YOLO11_Thumbnail.png)\n",
    "\n",
    "### Why Use Ultralytics?\n",
    "- **Ease of Use**: Simplified API for training and inference.\n",
    "- **State-of-the-art Models**: Access to cutting-edge models.\n",
    "- **Versatility**: Supports various tasks like object detection, segmentation, and classification.\n",
    "\n",
    "### What You'll Learn\n",
    "- How to load pre-trained models using Ultralytics.\n",
    "- Performing inference on images and videos.\n",
    "- Visualizing detection results.\n",
    "- Exploring different models and their performance.\n",
    "\n",
    "# üß∞ Environment Setup\n",
    "\n",
    "We'll use the Ultralytics library, which can be installed via pip. We'll also need some additional libraries for image handling and visualization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb8da32",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ultralytics matplotlib opencv-python requests pillow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576fbf37",
   "metadata": {},
   "source": [
    "### Import Libraries\n",
    "\n",
    "We import the necessary libraries for loading the model, processing images, and visualizing results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020ab9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO, RTDETR\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import requests\n",
    "from PIL import Image\n",
    "import io\n",
    "\n",
    "print(\"ultralytics models are ready for inference!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d16f45",
   "metadata": {},
   "source": [
    "# üì¶ Loading a Pre-trained Model\n",
    "\n",
    "We'll use a pre-trained YOLO model from Ultralytics. The model is pre-trained on large datasets like COCO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbd1fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pre-trained YOLOv8 model\n",
    "yolo5  = YOLO('yolov5n.pt')  # Load an official model\n",
    "yolo8  = YOLO('yolov8n.pt')\n",
    "yolo10 = YOLO('yolov10n.pt')\n",
    "yolo11 = YOLO('yolo11n.pt')\n",
    "rtdetr = RTDETR('rtdetr-l.pt')\n",
    "\n",
    "# Explanation\n",
    "# model: ultralytics contains a hub of models including versions of YOLO models\n",
    "# 'n' : 'nano' indicating a small variant suitable for general use.\n",
    "# you can choose bigger versions such as :\n",
    "# - 's' : 'small'\n",
    "# - 'm' : 'medium'\n",
    "# - 'l' : 'large'\n",
    "# - 'x' : 'extra large'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58548594",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "ultralytics contains a hub of models including versions of YOLO models\n",
    "\n",
    "'n' : 'nano' indicating a small variant suitable for general use.\n",
    "\n",
    "you can choose bigger versions such as :\n",
    " - 's' : 'small'\n",
    " - 'm' : 'medium'\n",
    " - 'l' : 'large'\n",
    " - 'x' : 'extra large'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a756633b",
   "metadata": {},
   "source": [
    "# üñºÔ∏è Image Preprocessing\n",
    "\n",
    "We need to preprocess an input image to match the model's expected input format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a69dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download a sample image\n",
    "url = 'https://ultralytics.com/images/bus.jpg'\n",
    "response = requests.get(url)\n",
    "img_bytes = io.BytesIO(response.content)\n",
    "img = Image.open(img_bytes)\n",
    "\n",
    "# Convert the image to a numpy array for OpenCV\n",
    "img_np = np.array(img)\n",
    "\n",
    "# Display the image\n",
    "plt.imshow(img_np)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# Explanation\n",
    "# img: The raw input image loaded from a URL.\n",
    "# img_np: The image converted to a numpy array for further processing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e414f66",
   "metadata": {},
   "source": [
    "# üîç Running Inference and Visualizing Results\n",
    "\n",
    "Pass the image through the model to get detection results and visualize them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b042f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some inference arguments\n",
    "inference_args = {\n",
    "    'conf': 0.25,    # Confidence threshold\n",
    "    'device': 'cpu', # Use 'cpu' or 'cuda:0' if GPU is available\n",
    "    'iou': 0.5,      # IoU threshold for NMS\n",
    "}\n",
    "\n",
    "results_yolo5  = yolo5(img_np, conf=inference_args['conf'], device=inference_args['device'], iou=inference_args['iou'])\n",
    "results_yolo8  = yolo8(img_np, conf=inference_args['conf'], device=inference_args['device'], iou=inference_args['iou'])\n",
    "results_yolo10 = yolo10(img_np, conf=inference_args['conf'], device=inference_args['device'], iou=inference_args['iou'])\n",
    "results_yolo11 = yolo11(img_np, conf=inference_args['conf'], device=inference_args['device'], iou=inference_args['iou'])\n",
    "results_rtdetr = rtdetr(img_np, conf=inference_args['conf'], device=inference_args['device'], iou=inference_args['iou'])\n",
    "\n",
    "# Explanation\",\n",
    "        # - results: Contains detection outputs (e.g., bounding boxes, scores) for each model.\",\n",
    "        # - Inference is non-destructive; the original image remains unchanged.\",\n",
    "        # - Models process the image independently, allowing for direct comparison\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e4b2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the results object\n",
    "print(\"results_yolo5[0] output:\")\n",
    "print(results_yolo5[0])\n",
    "\n",
    "\n",
    "# EXPLANATION of ultralytics.engine.results.Results object with attributes:\n",
    "# - boxes: Contains the detected objects' bounding boxes, confidence scores, and class indices.\n",
    "# - names: Maps class indices to their respective names.\n",
    "# - orig_img: The original image as a NumPy array.\n",
    "# - speed: Time taken for preprocessing, inference, and postprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6252cf4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the boxes attribute\n",
    "boxes = results_yolo5[0].boxes\n",
    "\n",
    "# Print the boxes object\n",
    "print(\"\\nresults_yolo5[0].boxes:\")\n",
    "print(boxes)\n",
    "\n",
    "# Extract and print class labels, confidence scores, and bounding boxes\n",
    "print(\"\\nClass indices:\", boxes.cls)\n",
    "print(\"Confidence scores:\", boxes.conf)\n",
    "print(\"Bounding boxes (xyxy format):\", boxes.xyxy)\n",
    "print(\"Bounding boxes (xywh format):\", boxes.xywh)\n",
    "print(\"Normalized bounding boxes (xyxyn format):\", boxes.xyxyn)\n",
    "\n",
    "# EXPLANATION:\n",
    "# cls: Class indices for each detected object.\n",
    "# conf: Confidence scores for each detected object.\n",
    "# xyxy: Bounding box coordinates in xyxy format.\n",
    "# xywh: Bounding box coordinates in xywh format.\n",
    "# xyxyn: Normalized bounding box coordinates in xyxy format.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b756db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot results\n",
    "def plot_results(results, title):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(cv2.cvtColor(results[0].plot(), cv2.COLOR_BGR2RGB))\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "plot_results(results_yolo5, 'YOLOv5')\n",
    "plot_results(results_yolo8, 'YOLOv8')\n",
    "plot_results(results_yolo10, 'YOLOv10')\n",
    "plot_results(results_yolo11, 'YOLOv11')\n",
    "plot_results(results_rtdetr, 'RT-DETR')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2730267",
   "metadata": {},
   "source": [
    "# üß† Interpreting the Results\n",
    "\n",
    "The output image shows detected objects with bounding boxes and confidence scores. Each detected object is labeled with a class name and a confidence score indicating the model's confidence in the detection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb84204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to print detection details\n",
    "def print_results(results, model_name):\n",
    "    print(f\"Results for {model_name}:\")\n",
    "    # Extract and print bounding boxes, labels, and confidence scores\n",
    "    for result in results:\n",
    "        boxes = result.boxes.xyxy.cpu().numpy()  # Bounding boxes in format xyxy\n",
    "        scores = result.boxes.conf.cpu().numpy() # Confidence scores\n",
    "        classes = result.boxes.cls.cpu().numpy() # Class indices\n",
    "\n",
    "        # Print each detection\n",
    "        for i, box in enumerate(boxes):\n",
    "            print(f\"  Object {i+1}:\")\n",
    "            print(f\"    BBox Coordinates: {box}\")\n",
    "            print(f\"    Class ID: {classes[i]}, Score: {scores[i]:.2f}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Print results for each model\n",
    "print_results(results_yolo5, 'YOLOv5')\n",
    "print_results(results_yolo8, 'YOLOv8')\n",
    "print_results(results_yolo10, 'YOLOv10')\n",
    "print_results(results_yolo11, 'YOLOv11')\n",
    "print_results(results_rtdetr, 'RT-DETR')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd40287",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3db3583a",
   "metadata": {},
   "source": [
    "# # # # # # # # # # # # # # # # # # # # # # # #\n",
    "\n",
    "# üí° Student Task\n",
    "\n",
    "Use images from the [Microsoft COCO dataset explorer](https://cocodataset.org/#explore).\n",
    "\n",
    "Tasks:\n",
    "1. Load a different image and run inference on it.\n",
    "2. Experiment with different YOLO models (e.g., YOLOv5, YOLOv8, YOLOv10) and compare their performance.\n",
    "3. Modify the confidence threshold to filter out low-confidence detections.\n",
    "4. *(**HOME**)* Try running inference on a video file or webcam feed.\n",
    "\n",
    "Tips:\n",
    "- For videos or webcam feed, refer to the Ultralytics documentation for examples.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c2b2eb",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "metrologue",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
