{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1FoOW9LVAyi8GCvXZ3QytHt6Jk6ssjRdA)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Author: \n",
        "- **Safouane El Ghazouali**, \n",
        "- Ph.D. in AI, \n",
        "- Senior data scientist and researcher at TOELT LLC,\n",
        "- Lecturer at HSLU\n",
        "\n",
        "# -----  -----  -----  -----  -----  -----  -----  -----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "intro_transformers"
      },
      "source": [
        "# Introduction to Transformers\n",
        "\n",
        "Welcome to this introductory notebook on Transformers! Transformers are a powerful architecture in machine learning, revolutionizing fields like natural language processing (NLP) and computer vision.\n",
        "\n",
        "In this notebook, we'll cover the basics of what transformers are, why they matter, and a high-level overview of their components.\n",
        "\n",
        "![Vision-Transformer](https://github.com/safouaneelg/HSLU-Transformers-in-Vision/blob/main/vit-course.png?raw=true)\n",
        "\n",
        "### Why Transformers?\n",
        "- **Efficiency in Handling Sequences**: Unlike RNNs, transformers process sequences in parallel.\n",
        "- **Attention Mechanism**: Allows models to focus on relevant parts of input.\n",
        "- **Scalability**: Powers large models like GPT and BERT."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "learning_objectives"
      },
      "source": [
        "### What You'll Learn\n",
        "- The history and motivation behind transformers.\n",
        "- Key components: Attention, Positional Encoding, Encoder-Decoder.\n",
        "- Applications in NLP and beyond."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "env_setup"
      },
      "source": [
        "# üß∞ Environment Setup\n",
        "\n",
        "We'll use PyTorch for this course. Install it if needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_torch"
      },
      "outputs": [],
      "source": [
        "!pip install -q torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imports"
      },
      "source": [
        "### Import Libraries\n",
        "\n",
        "Here we import PyTorch, which will be used throughout the notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "import_torch"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "print(\"PyTorch version:\", torch.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "history"
      },
      "source": [
        "# üìú History of Transformers\n",
        "\n",
        "Transformers were introduced in the paper \"Attention is All You Need\" by Vaswani et al. in 2017. They addressed limitations of RNNs and LSTMs in handling long sequences."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "key_innovation"
      },
      "source": [
        "### Key Innovation: Self-Attention\n",
        "\n",
        "Self-attention allows the model to weigh different parts of the input sequence dynamically."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "architecture"
      },
      "source": [
        "# üèóÔ∏è Transformer Architecture Overview\n",
        "\n",
        "A transformer consists of:\n",
        "- Encoder stack\n",
        "- Decoder stack\n",
        "- Multi-head attention layers\n",
        "- Feed-forward networks\n",
        "- Positional encodings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "positional_encoding"
      },
      "source": [
        "### Positional Encoding Explanation\n",
        "\n",
        "Since transformers don't process sequences recurrently, we add positional encodings to give order information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pos_enc_code"
      },
      "outputs": [],
      "source": [
        "# Simple positional encoding example\n",
        "def positional_encoding(position, d_model):\n",
        "    return torch.tensor([position / (10000 ** (2 * i / d_model)) for i in range(d_model)])\n",
        "\n",
        "# Explanation of the variable\n",
        "# positional_encoding: A function to compute sine/cosine-based positional encodings.\n",
        "# position: The position in the sequence.\n",
        "# d_model: The dimension of the model embeddings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "attention_mechanism"
      },
      "source": [
        "# The Attention Mechanism in Transformers\n",
        "\n",
        "This section dives deeper into the core of transformers: the attention mechanism. We'll explain self-attention and implement it in PyTorch.\n",
        "\n",
        "Author: Safouane El Ghazouali, Ph.D. in AI, Senior data scientist and researcher at TOELT LLC, Lecturer at HSLU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "learning_objectives_2"
      },
      "source": [
        "### What You'll Learn\n",
        "- How attention works.\n",
        "- Scaled dot-product attention.\n",
        "- Multi-head attention."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "env_setup_2"
      },
      "source": [
        "# üß∞ Environment Setup\n",
        "\n",
        "Reuse PyTorch from previous section."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imports_2"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "understanding_attention"
      },
      "source": [
        "# üîç Understanding Attention\n",
        "\n",
        "Attention computes a weighted sum of values based on similarity between queries and keys."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qkv_explain"
      },
      "source": [
        "### Queries, Keys, Values\n",
        "\n",
        "- Queries: What we're looking for.\n",
        "- Keys: What we compare against.\n",
        "- Values: What we retrieve."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qkv_code"
      },
      "outputs": [],
      "source": [
        "# Example tensors\n",
        "queries = torch.randn(2, 3)  # Batch size 2, embedding dim 3\n",
        "keys = torch.randn(2, 3)\n",
        "values = torch.randn(2, 3)\n",
        "\n",
        "print(\"queries\", queries)\n",
        "print(\"keys\", keys)\n",
        "print(\"values\", values)\n",
        "\n",
        "# Explanation of variables\n",
        "# queries: Represents the query vectors in attention.\n",
        "# keys: Key vectors used to compute attention scores.\n",
        "# values: Value vectors that are weighted and summed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scaled_dot"
      },
      "source": [
        "# üìê Scaled Dot-Product Attention\n",
        "\n",
        "The formula: $Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "attention_impl"
      },
      "outputs": [],
      "source": [
        "def scaled_dot_product_attention(q, k, v):\n",
        "    d_k = q.size(-1)\n",
        "    scores = torch.matmul(q, k.T) / torch.sqrt(torch.tensor(d_k).float())\n",
        "    attn = F.softmax(scores, dim=-1)\n",
        "    return torch.matmul(attn, v)\n",
        "\n",
        "# Compute attention\n",
        "output = scaled_dot_product_attention(queries, keys, values)\n",
        "print(output)\n",
        "\n",
        "# Explanation\n",
        "# scores: Dot product of queries and keys, scaled.\n",
        "# attn: Softmax probabilities.\n",
        "# output: Weighted sum of values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hands_on_timm"
      },
      "source": [
        "# Hands-on: Pre-trained Transformer Models for Image Classification\n",
        "\n",
        "In this section, we'll use PyTorch and the timm library to load pre-trained Vision Transformer (ViT) models and perform inference on images.\n",
        "\n",
        "Author: Safouane El Ghazouali, Ph.D. in AI, Senior data scientist and researcher at TOELT LLC, Lecturer at HSLU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "learning_objectives_3"
      },
      "source": [
        "### What You'll Learn\n",
        "- Installing and using timm.\n",
        "- Loading pre-trained ViT models.\n",
        "- Running inference on sample images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "env_setup_3"
      },
      "source": [
        "# üß∞ Environment Setup\n",
        "\n",
        "Install timm and other dependencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_timm"
      },
      "outputs": [],
      "source": [
        "!pip install -q timm torch torchvision"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imports_3"
      },
      "source": [
        "### Import Libraries\n",
        "\n",
        "Import timm for models, torchvision for image handling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "import_timm"
      },
      "outputs": [],
      "source": [
        "import timm\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import requests"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "load_model"
      },
      "source": [
        "# üì¶ Loading a Pre-trained Model\n",
        "\n",
        "We'll load a Vision Transformer model pre-trained on ImageNet."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create_model"
      },
      "outputs": [],
      "source": [
        "model = timm.create_model('vit_base_patch16_224', pretrained=True)\n",
        "model.eval()\n",
        "\n",
        "# Explanation\n",
        "# model: A pre-trained ViT model.\n",
        "# eval(): Sets the model to evaluation mode."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "preprocess_image"
      },
      "source": [
        "# üñºÔ∏è Image Preprocessing\n",
        "\n",
        "Prepare an image for input to the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "image_transform"
      },
      "outputs": [],
      "source": [
        "# Download a sample image\n",
        "url = 'http://farm8.staticflickr.com/7012/6597749473_03b2f736ac_z.jpg'\n",
        "img = Image.open(requests.get(url, stream=True).raw)\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "input_tensor = transform(img).unsqueeze(0)\n",
        "input_tensor.shape\n",
        "# Explanation\n",
        "# transform: Preprocessing pipeline to resize, normalize the image.\n",
        "# input_tensor: The image as a tensor ready for model input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "img"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inference"
      },
      "source": [
        "# üöÄ Running Inference\n",
        "\n",
        "Pass the image through the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "run_inference"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    output = model(input_tensor)\n",
        "probabilities = torch.nn.functional.softmax(output[0], dim=0)\n",
        "print(probabilities)\n",
        "\n",
        "# Explanation\n",
        "# output: Logits from the model.\n",
        "# probabilities: Softmax probabilities for each class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "url = \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\"\n",
        "imagenet_classes = requests.get(url).text.strip().split('\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "top10_prob, top10_catid = torch.topk(probabilities, 10)\n",
        "\n",
        "print(\"\\n--- Top 10 Predictions ---\")\n",
        "for i in range(top10_prob.size(0)):\n",
        "    print(f\"{i+1}. Class: {imagenet_classes[top10_catid[i]]}, Probability: {top10_prob[i].item():.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "student_task_3"
      },
      "source": [
        "# # # # # # # # # # # # # # # # # # # # # # # #\n",
        "# üí° Student Task\n",
        "\n",
        "In this task you can use image `URLs` from Microsoft coco data explorer\n",
        "\n",
        "Task:\n",
        "\n",
        "1. Download/look at a different image, preprocess it, and run inference.  \n",
        "\n",
        "2. What class does the model predict?\n",
        "\n",
        "3. Load other models from the `timm` library  such as ResNet, VGG. \n",
        "\n",
        "4. run the inference on the image and print probabilities\n",
        "\n",
        "5. print the top 10 classes\n",
        "\n",
        "6. Compare the performances\n",
        "\n",
        "Tips: use `timm.list_models(filter=\"*{MODEL_NAME}*\")` to print all available models in the timm hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model2 = timm.create_model('resnet50', pretrained=True)\n",
        "model2.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    output = model2(input_tensor)\n",
        "    # The output is a tensor of shape [1, 1000]. We apply softmax to get probabilities.\n",
        "probabilities = torch.nn.functional.softmax(output, dim=-1)[0] # [0] to get rid of the batch dimension\n",
        "print(probabilities.shape)\n",
        "top5_prob, top5_catid = torch.topk(probabilities, 5)\n",
        "\n",
        "print(\"\\n--- Top 5 Predictions ---\")\n",
        "for i in range(top5_prob.size(0)):\n",
        "    class_name = imagenet_classes[top5_catid[i]]\n",
        "    probability = top5_prob[i].item()\n",
        "    print(f\"{i+1}. Class: {class_name}, Probability: {probability:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "author": "Safouane El Ghazouali, Ph.D. in AI, Senior data scientist and researcher at TOELT LLC, Lecturer at HSLU",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
