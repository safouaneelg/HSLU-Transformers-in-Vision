{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ol6X_t7SsS0v"
      },
      "source": [
        "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1untfZXgw5QVkyVxNt7AtxS5zyd6mz5wn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QyjAyuvZsS0x"
      },
      "source": [
        "Author:\n",
        "- **Safouane El Ghazouali**,\n",
        "- Ph.D. in AI,\n",
        "- Senior data scientist and researcher at TOELT LLC,\n",
        "- Lecturer at HSLU\n",
        "\n",
        "# -----  -----  -----  -----  -----  -----  -----  -----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "diffusion_intro"
      },
      "source": [
        "# Hands-on: Diffusion Models with Hugging Face Diffusers\n",
        "\n",
        "Welcome to this comprehensive hands-on notebook on using diffusion models with the Hugging Face Diffusers library! We'll focus on small, efficient Stable Diffusion models for various use cases, including text-to-image generation, inpainting, and more. This tutorial teaches how to load models from Hugging Face and apply them practically.\n",
        "\n",
        "![Diffusion Example](https://learnopencv.com/wp-content/uploads/2023/01/diffusion-models-unconditional_image_generation-1.gif)\n",
        "\n",
        "### Why Use Diffusion Models?\n",
        "- **Generative Power**: Create realistic images from text descriptions.\n",
        "- **Versatility**: Supports generation, editing (inpainting), variation, and guided synthesis.\n",
        "- **Efficiency with Small Models**: Use lightweight variants for faster inference on limited hardware.\n",
        "- **Hugging Face Ecosystem**: Easy access to pre-trained models and pipelines.\n",
        "\n",
        "### What You'll Learn\n",
        "- Installing and setting up Diffusers.\n",
        "- Loading small Stable Diffusion models from Hugging Face.\n",
        "- Text-to-image generation.\n",
        "- Inpainting (filling masked regions).\n",
        "- Image-to-image variation.\n",
        "- ControlNet for structure-guided generation (optional, if resources allow).\n",
        "- Tips for prompt engineering and parameter tuning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "env_setup"
      },
      "source": [
        "# üß∞ Environment Setup\n",
        "\n",
        "Install Diffusers and dependencies. Use torch with CUDA if available for GPU acceleration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_deps"
      },
      "outputs": [],
      "source": [
        "!pip install -q diffusers transformers accelerate torch torchvision requests pillow matplotlib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imports"
      },
      "source": [
        "### Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "import_libs"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from diffusers import StableDiffusionPipeline, StableDiffusionInpaintPipeline, StableDiffusionImg2ImgPipeline, ControlNetModel, StableDiffusionControlNetPipeline\n",
        "import requests\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "torch_dtype = torch.float16 if device == 'cuda' else torch.float32\n",
        "print(f'Using device: {device} with dtype: {torch_dtype}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "diffusion_explain"
      },
      "source": [
        "# üìö Understanding Diffusion Models\n",
        "\n",
        "Diffusion models generate images by iteratively denoising random noise guided by text prompts. Stable Diffusion is a popular latent diffusion model.\n",
        "\n",
        "- **Small Models**: We'll use 'runwayml/stable-diffusion-v1-5' for balance; 'stabilityai/sd-turbo' for speed.\n",
        "- **Pipelines**: Diffusers provides ready-to-use pipelines for different tasks.\n",
        "- **Parameters**: num_inference_steps (quality vs. speed), guidance_scale (adherence to prompt).\n",
        "\n",
        "Reference: [Hugging Face Diffusers](https://huggingface.co/docs/diffusers/index)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "load_model"
      },
      "source": [
        "# üì¶ Loading a Small Stable Diffusion Model\n",
        "\n",
        "Load the base pipeline for text-to-image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "load_sd"
      },
      "outputs": [],
      "source": [
        "model_id = 'runwayml/stable-diffusion-v1-5'\n",
        "pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch_dtype).to(device)\n",
        "print('Stable Diffusion loaded!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "text_to_image"
      },
      "source": [
        "# üñºÔ∏è Text-to-Image Generation\n",
        "\n",
        "Generate images from text prompts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t2i_code"
      },
      "outputs": [],
      "source": [
        "prompt = 'A futuristic cityscape at sunset, cyberpunk style'\n",
        "negative_prompt = 'blurry, low quality'\n",
        "\n",
        "image_gen = pipe(\n",
        "    prompt,\n",
        "    negative_prompt=negative_prompt,\n",
        "    num_inference_steps=20,  # Small for speed\n",
        "    guidance_scale=7.5\n",
        ").images[0]\n",
        "\n",
        "plt.imshow(image_gen)\n",
        "plt.title('Generated Image')\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "# Explanation\n",
        "# - prompt: Describes desired image.\n",
        "# - negative_prompt: Avoids unwanted elements.\n",
        "# - guidance_scale: Higher values follow prompt closely."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inpainting"
      },
      "source": [
        "# üé® Inpainting: Editing Images\n",
        "\n",
        "Fill masked regions based on prompts. First, create a mask."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "inpaint_code"
      },
      "outputs": [],
      "source": [
        "img_url = \"https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo.png\"\n",
        "mask_url = \"https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo_mask.png\"\n",
        "# Load inpaint pipeline\n",
        "inpaint_pipe = StableDiffusionInpaintPipeline.from_pretrained(\n",
        "    model_id,\n",
        "    torch_dtype=torch_dtype\n",
        ").to(device)\n",
        "# Sample image and mask\n",
        "init_image = Image.open(BytesIO(requests.get(img_url).content)).convert('RGB')\n",
        "mask_image = Image.open(BytesIO(requests.get(mask_url).content)).convert('RGB')\n",
        "# Display init and mask\n",
        "fig, axs = plt.subplots(1, 2)\n",
        "axs[0].imshow(init_image)\n",
        "axs[0].set_title('Initial Image')\n",
        "axs[1].imshow(mask_image)\n",
        "axs[1].set_title('Mask')\n",
        "plt.show()\n",
        "# Inpaint\n",
        "prompt_inpaint = 'An orange cat'\n",
        "image_inpaint = inpaint_pipe(\n",
        "    prompt=prompt_inpaint,\n",
        "    image=init_image,\n",
        "    mask_image=mask_image,\n",
        "    num_inference_steps=50,\n",
        "    guidance_scale=9.5\n",
        ").images[0]\n",
        "plt.imshow(image_inpaint)\n",
        "plt.title('Inpainted Image')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "img2img"
      },
      "source": [
        "# üîÑ Image-to-Image Variation\n",
        "\n",
        "Generate variations of an input image guided by a prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i2i_code"
      },
      "outputs": [],
      "source": [
        "# Load img2img pipeline\n",
        "img2img_pipe = StableDiffusionImg2ImgPipeline.from_pretrained(\n",
        "    model_id,\n",
        "    torch_dtype=torch_dtype\n",
        ").to(device)\n",
        "\n",
        "# Use init_image from above\n",
        "prompt_i2i = 'A cyberpunk version of the girl with a pearl earring'\n",
        "image_i2i = img2img_pipe(\n",
        "    prompt=prompt_i2i,\n",
        "    image=init_image.resize((512, 512)),\n",
        "    strength=0.75,  # How much to change\n",
        "    num_inference_steps=20,\n",
        "    guidance_scale=7.5\n",
        ").images[0]\n",
        "\n",
        "plt.imshow(image_i2i)\n",
        "plt.title('Image-to-Image Variation')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "controlnet"
      },
      "source": [
        "# üïπÔ∏è ControlNet: Structure-Guided Generation\n",
        "\n",
        "Use ControlNet for edge-guided generation (requires additional model)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "controlnet_code"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "\n",
        "# Load ControlNet\n",
        "controlnet = ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-canny', torch_dtype=torch_dtype)\n",
        "control_pipe = StableDiffusionControlNetPipeline.from_pretrained(\n",
        "    model_id,\n",
        "    controlnet=controlnet,\n",
        "    torch_dtype=torch_dtype\n",
        ").to(device)\n",
        "\n",
        "# Generate Canny edges from init_image\n",
        "init_np = np.array(init_image.resize((512, 512)))\n",
        "canny_image = cv2.Canny(init_np, 100, 200)\n",
        "canny_image = Image.fromarray(canny_image)\n",
        "\n",
        "# Display Canny\n",
        "plt.imshow(canny_image, cmap='gray')\n",
        "plt.title('Canny Edges')\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "# Generate\n",
        "prompt_control = 'A colorful abstract painting of a woman'\n",
        "image_control = control_pipe(\n",
        "    prompt_control,\n",
        "    image=canny_image,\n",
        "    num_inference_steps=20,\n",
        "    guidance_scale=7.5\n",
        ").images[0]\n",
        "\n",
        "plt.imshow(image_control)\n",
        "plt.title('ControlNet Generation')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "other_models"
      },
      "source": [
        "# üîÑ Using Other Small Models (e.g., SD-Turbo)\n",
        "\n",
        "Load SD-Turbo for faster generation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "turbo_code"
      },
      "outputs": [],
      "source": [
        "turbo_id = 'stabilityai/sd-turbo'\n",
        "turbo_pipe = StableDiffusionPipeline.from_pretrained(turbo_id, torch_dtype=torch_dtype).to(device)\n",
        "\n",
        "prompt_turbo = 'A serene mountain landscape'\n",
        "image_turbo = turbo_pipe(\n",
        "    prompt_turbo,\n",
        "    num_inference_steps=1,  # Turbo is fast!\n",
        "    guidance_scale=0.0  # Classifier-free\n",
        ").images[0]\n",
        "\n",
        "plt.imshow(image_turbo)\n",
        "plt.title('SD-Turbo Generation')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}